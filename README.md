# ğŸ™ï¸ Emotion Recognition from Speech using Deep Learning

![Python](https://img.shields.io/badge/Python-3.9+-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-Deep%20Learning-orange)
![Librosa](https://img.shields.io/badge/Librosa-Audio%20Processing-green)
![Status](https://img.shields.io/badge/Status-Completed-success)

---

## ğŸ“Œ Project Overview

Human emotions play a crucial role in communication. This project focuses on **automatic emotion recognition from speech audio** using **speech signal processing** and **deep learning** techniques.

The system extracts **MFCC (Mel-Frequency Cepstral Coefficients)** from speech signals and uses a **Convolutional Neural Network (CNN)** to classify emotions.

This project is developed as part of **CodeAlpha Internship â€“ Task 2**.

---

## ğŸ¯ Objectives

- Extract meaningful features from speech signals  
- Train a deep learning model to classify emotions  
- Evaluate model accuracy on unseen data  
- Predict emotions from new speech audio  

---

## ğŸ§  Emotions Classified

The model recognizes the following **8 emotions**:

- ğŸ˜ Neutral  
- ğŸ˜Œ Calm  
- ğŸ˜Š Happy  
- ğŸ˜¢ Sad  
- ğŸ˜  Angry  
- ğŸ˜¨ Fear  
- ğŸ¤¢ Disgust  
- ğŸ˜² Surprise  

---

## ğŸ§ª Dataset Used

### ğŸ¼ RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)

- 1440+ labeled speech samples  
- 24 professional actors  
- High-quality `.wav` audio files  
- Emotion labels encoded in filenames  

### Why RAVDESS?
- Widely used academic dataset  
- Balanced emotion classes  
- Clean and standardized recordings  

---

## ğŸ“ Project Structure

## ğŸ“ Project Structure

```text
CodeAlpha_Emotion_Recognition_from_Speech/
â”‚
â”œâ”€â”€ RAVDESS
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ emotion_model.h5
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract_features.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ evaluate.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

```
---
## âš™ï¸ Technologies Used

### ğŸ”¹ Programming Language
- Python 3.9+

### ğŸ”¹ Libraries & Frameworks
- TensorFlow / Keras â€“ Deep Learning  
- Librosa â€“ Speech & audio processing  
- NumPy â€“ Numerical computations  
- Scikit-learn â€“ Label encoding & evaluation  
- Matplotlib / Seaborn â€“ Visualization  

---

## ğŸ” Feature Extraction

### ğŸµ MFCC (Mel-Frequency Cepstral Coefficients)

MFCCs are used because:
- They closely represent human auditory perception  
- Effective for speech and emotion recognition  
- Reduce noise and irrelevant information  

Each audio file is converted into **40 MFCC features**.

---

## ğŸ§  Model Architecture

### ğŸ“Œ Convolutional Neural Network (CNN)

**Why CNN?**
- Learns spatial patterns from MFCC features  
- Faster training with fewer parameters  
- Strong performance on audio-based tasks  

---

## ğŸ“Š Model Performance

- Train/Test Split: **80% / 20%**
- Evaluation Metric: **Accuracy**
- Achieved Accuracy: **~70â€“75%**

> Accuracy may vary slightly due to random initialization and data splitting.

---
## ğŸš€ How to Run the Project

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Shubhra-kanti/CodeAlpha_Emotion_Recognition_from_Speech.git
cd CodeAlpha_Emotion_Recognition_from_Speech
```
2ï¸âƒ£ Create and Activate Virtual Environment
```bash
python -m venv myenv
myenv\Scripts\activate
```
3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```
4ï¸âƒ£ Train the Model
```bash
python src/train_model.py
```
5ï¸âƒ£ Test Emotion on New Audio
```bash
python src/test_audio.py
```
ğŸ§ª Sample Output
Predicted Emotion: Happy ğŸ˜Š

ğŸ”® Future Enhancements

Data augmentation for improved accuracy

LSTM / BiLSTM models for temporal learning

Web interface using Flask or Streamlit

Real-time emotion recognition

ğŸ‘¨â€ğŸ’» Author

Shubhra Kanti Banerjee
Engineering Student
West Bengal, India

ğŸ“œ License

This project is intended for educational and research purposes only.
